{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step: Load data and packages"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext lab_black"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/mhanauer60/code/Users/mhanauer/ml-text-cleaning-py\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyodbc\n",
        "import numpy as np\n",
        "from skimpy import clean_columns\n",
        "from pyprojroot import here\n",
        "import os\n",
        "import docx\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "path_data = here(\"./data\")\n",
        "os.chdir(path_data)\n",
        "\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "workspace = Workspace.from_config()\n",
        "\n",
        "datastore = workspace.get_default_datastore()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'docx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyprojroot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m here\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1682628964697
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Combine text by case id, and lower text"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import docx\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "\n",
        "def read_pdf_file(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            text += reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def read_word_file(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text\n",
        "    return text"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681824366788
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_path = \"./CV_Matt_Hanauer.docx\"\n",
        "resume_pdf_path = \"./resume_matt_hanauer.pdf\"\n",
        "resume_path = \"./resume_matt_hanauer.docx\"\n",
        "job_description_path = \"./job_example.docx\"\n",
        "\n",
        "resume_text = read_word_file(resume_path)\n",
        "resume_pdf = read_pdf_file(resume_pdf_path)\n",
        "cv_text = read_word_file(cv_path)\n",
        "job_description_text = read_word_file(job_description_path)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681824371865
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words_filtered = [word for word in words if word.isalnum()]\n",
        "    return words_filtered\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "\n",
        "def compare_texts(text1_tokens, text2_tokens, top_n_scores=10):\n",
        "    text1_sentences = \" \".join(text1_tokens)\n",
        "    text2_sentences = \" \".join(text2_tokens)\n",
        "\n",
        "    text1_embedding = model.encode(text1_sentences, convert_to_tensor=True)\n",
        "    text2_embedding = model.encode(text2_sentences, convert_to_tensor=True)\n",
        "\n",
        "    similarity_score = cosine_similarity(text1_embedding, text2_embedding).round(2)\n",
        "    return similarity_score"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681826072107
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_texts(resume_text, job_description_text)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "0.81"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681824397661
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_texts(resume_pdf, job_description_text)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "0.84"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681824399392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_texts(cv_text, job_description_text)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "0.89"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681824400740
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Identify which key words are missing"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Load the English stop words\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def calculate_word_frequency(text_tokens):\n",
        "    filtered_tokens = [token for token in text_tokens if token not in stop_words]\n",
        "    fdist = FreqDist(filtered_tokens)\n",
        "    return fdist"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681826081700
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the texts\n",
        "resume_tokens = preprocess_text(resume_text)\n",
        "job_description_tokens = preprocess_text(job_description_text)"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681826084257
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate frequency distributions\n",
        "resume_fdist = calculate_word_frequency(resume_tokens)\n",
        "job_description_fdist = calculate_word_frequency(job_description_tokens)"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681826086131
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 10 words from the job description\n",
        "top_n = 10\n",
        "top_job_description_words = [\n",
        "    word for word, freq in job_description_fdist.most_common(top_n)\n",
        "]\n",
        "\n",
        "# Find the missing top 10 words in the resume\n",
        "missing_keywords = []\n",
        "for word in top_job_description_words:\n",
        "    if word not in resume_fdist:\n",
        "        missing_keywords.append(word)\n",
        "\n",
        "missing_keywords"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "['affairs', 'gsk', 'patients', 'innovative']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681826087739
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "98def2bbe4c17ca6f7446c23f6cb6fc94941e782507d089d8d0095a990429397"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}